{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dabb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageFilter\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration - Key improvements\n",
    "DATA_DIR = \"./\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train/train/\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test/test/\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "\n",
    "BATCH_SIZE = 32  # Increased for better GPU utilization\n",
    "EPOCHS = 20  # More epochs with better early stopping\n",
    "LR = 5e-5  # Slightly lower learning rate\n",
    "N_SPLITS = 7  # More folds for better validation\n",
    "IMG_SIZE = 384  # Larger image size for better feature extraction\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['label_idx'] = le.fit_transform(df['TARGET'])\n",
    "df['image_id'] = df['ID']\n",
    "num_classes = df['label_idx'].nunique()\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Class distribution:\")\n",
    "print(df['TARGET'].value_counts())\n",
    "\n",
    "# Enhanced dataset with multiple augmentation strategies\n",
    "class AdvancedDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, is_training=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['image_id'])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # Apply subtle preprocessing\n",
    "            if self.is_training and np.random.random() < 0.1:\n",
    "                image = image.filter(ImageFilter.GaussianBlur(radius=0.5))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), color=(128, 128, 128))\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = row['label_idx']\n",
    "        return image, label\n",
    "\n",
    "# More aggressive and diverse augmentations\n",
    "def get_train_transforms(img_size=384):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((int(img_size * 1.1), int(img_size * 1.1))),\n",
    "        transforms.RandomCrop((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.15),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(img_size=384):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "# Test Time Augmentation transforms\n",
    "def get_tta_transforms(img_size=384):\n",
    "    return [\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((int(img_size * 1.05), int(img_size * 1.05))),\n",
    "            transforms.CenterCrop((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "    ]\n",
    "\n",
    "# Enhanced model with better architectures\n",
    "def get_model(model_name='efficientnet', num_classes=20):\n",
    "    if model_name == 'efficientnet':\n",
    "        from torchvision.models import efficientnet_b3\n",
    "        model = efficientnet_b3(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        print(\"✅ Using EfficientNet-B3\")\n",
    "    elif model_name == 'resnext101':\n",
    "        model = models.resnext101_32x8d(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        print(\"✅ Using ResNeXt101\")\n",
    "    elif model_name == 'densenet':\n",
    "        model = models.densenet161(pretrained=True)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "        print(\"✅ Using DenseNet161\")\n",
    "    else:\n",
    "        # Enhanced ResNeXt with dropout\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(model.fc.in_features, num_classes)\n",
    "        )\n",
    "        print(\"✅ Using Enhanced ResNeXt50\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Label smoothing loss\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        confidence = 1. - self.smoothing\n",
    "        log_probs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        \n",
    "        if self.weight is not None:\n",
    "            loss = loss * self.weight[target]\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "# Calculate class weights with smoothing\n",
    "def get_class_weights(df, smoothing=0.1):\n",
    "    class_counts = df['label_idx'].value_counts().sort_index().values\n",
    "    total_samples = len(df)\n",
    "    # Apply smoothing to avoid extreme weights\n",
    "    smoothed_counts = class_counts + smoothing * total_samples / len(class_counts)\n",
    "    class_weights = total_samples / (len(smoothed_counts) * smoothed_counts)\n",
    "    # Cap extreme weights\n",
    "    class_weights = np.clip(class_weights, 0.5, 3.0)\n",
    "    return torch.FloatTensor(class_weights)\n",
    "\n",
    "# Training function with gradient accumulation\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, accumulation_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (imgs, labels) in enumerate(pbar):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels) / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * imgs.size(0) * accumulation_steps\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item() * accumulation_steps:.4f}', \n",
    "                'Acc': f'{correct/total:.4f}',\n",
    "                'Batch': f'{batch_idx}/{len(loader)}'\n",
    "            })\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# Enhanced validation with TTA\n",
    "def validate_with_tta(model, dataset, device, batch_size=32):\n",
    "    model.eval()\n",
    "    tta_transforms = get_tta_transforms(IMG_SIZE)\n",
    "    \n",
    "    all_preds = []\n",
    "    labels_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(dataset.df)), desc=\"TTA Validation\"):\n",
    "            row = dataset.df.iloc[idx]\n",
    "            img_path = os.path.join(dataset.img_dir, row['image_id'])\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "            except:\n",
    "                image = Image.new('RGB', (224, 224), color=(128, 128, 128))\n",
    "            \n",
    "            tta_preds = []\n",
    "            for transform in tta_transforms:\n",
    "                img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "                output = model(img_tensor)\n",
    "                pred = F.softmax(output, dim=1)\n",
    "                tta_preds.append(pred.cpu().numpy())\n",
    "            \n",
    "            # Average TTA predictions\n",
    "            avg_pred = np.mean(tta_preds, axis=0)\n",
    "            all_preds.append(avg_pred)\n",
    "            labels_all.append(row['label_idx'])\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    pred_classes = np.argmax(all_preds, axis=1)\n",
    "    \n",
    "    f1 = f1_score(labels_all, pred_classes, average=\"micro\")\n",
    "    accuracy = (pred_classes == np.array(labels_all)).mean()\n",
    "    \n",
    "    return f1, accuracy\n",
    "\n",
    "# Regular validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds_all, labels_all = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "            \n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(labels_all, preds_all, average=\"micro\")\n",
    "    return total_loss / total, correct / total, f1\n",
    "\n",
    "# Main training loop with multiple models\n",
    "def train_models():\n",
    "    # Use better performing models\n",
    "    model_configs = [\n",
    "        {'name': 'efficientnet', 'lr': 3e-5, 'epochs': 25},\n",
    "        {'name': 'resnext101', 'lr': 2e-5, 'epochs': 20},\n",
    "        {'name': 'densenet', 'lr': 4e-5, 'epochs': 22},\n",
    "    ]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for config in model_configs:\n",
    "        model_name = config['name']\n",
    "        lr = config['lr']\n",
    "        epochs = config['epochs']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label_idx'])):\n",
    "            print(f\"\\n🔥 Fold {fold+1}/{N_SPLITS}\")\n",
    "            \n",
    "            train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "            val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "            \n",
    "            # Create datasets\n",
    "            train_dataset = AdvancedDataset(train_df, TRAIN_DIR, get_train_transforms(IMG_SIZE), is_training=True)\n",
    "            val_dataset = AdvancedDataset(val_df, TRAIN_DIR, get_val_transforms(IMG_SIZE))\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                drop_last=True\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            # Model setup\n",
    "            model = get_model(model_name, num_classes).to(DEVICE)\n",
    "            \n",
    "            # Enhanced optimizer with different parameters\n",
    "            if model_name == 'efficientnet':\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "            else:\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-5)\n",
    "            \n",
    "            # Cosine annealing with warm restarts\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=5, T_mult=2, eta_min=1e-7\n",
    "            )\n",
    "            \n",
    "            # Enhanced loss\n",
    "            class_weights = get_class_weights(train_df).to(DEVICE)\n",
    "            criterion = LabelSmoothingCrossEntropy(smoothing=0.1, weight=class_weights)\n",
    "            \n",
    "            # Training loop\n",
    "            best_f1 = 0\n",
    "            patience = 7\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "                \n",
    "                # Train with gradient accumulation if batch size is small\n",
    "                accumulation_steps = max(1, 64 // BATCH_SIZE)\n",
    "                train_loss, train_acc = train_one_epoch(\n",
    "                    model, train_loader, optimizer, criterion, DEVICE, accumulation_steps\n",
    "                )\n",
    "                \n",
    "                # Validate\n",
    "                val_loss, val_acc, val_f1 = validate(model, val_loader, criterion, DEVICE)\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "                print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "                \n",
    "                # Enhanced TTA validation for best models\n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Use TTA for final validation on promising models\n",
    "                    if val_f1 > 0.88:\n",
    "                        tta_f1, tta_acc = validate_with_tta(model, val_dataset, DEVICE)\n",
    "                        print(f\"TTA - Acc: {tta_acc:.4f}, F1: {tta_f1:.4f}\")\n",
    "                        if tta_f1 > best_f1:\n",
    "                            best_f1 = tta_f1\n",
    "                    \n",
    "                    torch.save(model.state_dict(), f\"{model_name}_fold{fold}_best.pth\")\n",
    "                    print(f\"✅ New best F1: {best_f1:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"⏰ Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "            \n",
    "            fold_scores.append(best_f1)\n",
    "            print(f\"\\n📊 Fold {fold+1} completed - Best F1: {best_f1:.4f}\")\n",
    "        \n",
    "        avg_score = np.mean(fold_scores)\n",
    "        std_score = np.std(fold_scores)\n",
    "        all_results[model_name] = {\n",
    "            'scores': fold_scores,\n",
    "            'mean': avg_score,\n",
    "            'std': std_score\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n🎯 {model_name} Results:\")\n",
    "        print(f\"Fold scores: {[f'{score:.4f}' for score in fold_scores]}\")\n",
    "        print(f\"Average F1: {avg_score:.4f} ± {std_score:.4f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Ensemble prediction function\n",
    "def create_ensemble_predictions():\n",
    "    print(\"\\n🔮 Creating ensemble predictions...\")\n",
    "    \n",
    "    # Load test data info (you'll need to implement this based on test structure)\n",
    "    test_files = os.listdir(TEST_DIR)\n",
    "    \n",
    "    model_configs = [\n",
    "        {'name': 'efficientnet', 'weight': 0.4},\n",
    "        {'name': 'resnext101', 'weight': 0.35},  \n",
    "        {'name': 'densenet', 'weight': 0.25}\n",
    "    ]\n",
    "    \n",
    "    # Implementation would continue here for ensemble predictions...\n",
    "    print(\"Ensemble prediction framework ready!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting enhanced training...\")\n",
    "    results = train_models()\n",
    "    \n",
    "    print(\"\\n🎉 Training completed!\")\n",
    "    print(\"\\n📊 Final Results Summary:\")\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"{model_name}: {result['mean']:.4f} ± {result['std']:.4f}\")\n",
    "    \n",
    "    create_ensemble_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b238ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageFilter\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import gc\n",
    "\n",
    "# Configuration optimized for RTX 4060 (8GB VRAM)\n",
    "DATA_DIR = \"./\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train/train/\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test/test/\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "\n",
    "# RTX 4060 Optimized Settings\n",
    "BATCH_SIZE = 16  # Conservative for 8GB VRAM\n",
    "EPOCHS = 18\n",
    "LR = 3e-5\n",
    "N_SPLITS = 5  # Reduced for faster training\n",
    "IMG_SIZE = 320  # Balance between quality and memory\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "# Memory optimization\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Create model save directories\n",
    "MODEL_SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "for model_type in ['efficientnet', 'resnext101', 'densenet']:\n",
    "    model_dir = os.path.join(MODEL_SAVE_DIR, model_type)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['label_idx'] = le.fit_transform(df['TARGET'])\n",
    "df['image_id'] = df['ID']\n",
    "num_classes = df['label_idx'].nunique()\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Class distribution:\")\n",
    "print(df['TARGET'].value_counts())\n",
    "\n",
    "# Memory-efficient dataset\n",
    "class MemoryEfficientDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, is_training=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['image_id'])\n",
    "        \n",
    "        try:\n",
    "            # Load and immediately process to save memory\n",
    "            with Image.open(img_path) as image:\n",
    "                image = image.convert(\"RGB\")\n",
    "                \n",
    "                # Minimal preprocessing to save memory\n",
    "                if self.is_training and np.random.random() < 0.05:\n",
    "                    image = image.filter(ImageFilter.GaussianBlur(radius=0.3))\n",
    "                    \n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Create minimal fallback\n",
    "            blank = Image.new('RGB', (224, 224), color=(128, 128, 128))\n",
    "            image = self.transform(blank) if self.transform else blank\n",
    "            \n",
    "        label = row['label_idx']\n",
    "        return image, label\n",
    "\n",
    "# Optimized augmentations for 4060\n",
    "def get_train_transforms(img_size=320):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((int(img_size * 1.05), int(img_size * 1.05))),\n",
    "        transforms.RandomCrop((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.25),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.25)),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(img_size=320):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "# TTA transforms (memory efficient)\n",
    "def get_tta_transforms(img_size=320):\n",
    "    return [\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "    ]\n",
    "\n",
    "# Memory-optimized model loading\n",
    "def get_model(model_name='efficientnet', num_classes=20):\n",
    "    # Clear GPU cache before loading new model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    if model_name == 'efficientnet':\n",
    "        # Use smaller EfficientNet for 4060\n",
    "        try:\n",
    "            model = models.efficientnet_b2(pretrained=True)  # B2 instead of B3 for memory\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "            print(\"✅ Using EfficientNet-B2 (Memory Optimized)\")\n",
    "        except:\n",
    "            model = models.efficientnet_b0(pretrained=True)\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "            print(\"✅ Using EfficientNet-B0 (Fallback)\")\n",
    "    elif model_name == 'resnext101':\n",
    "        # Use ResNeXt50 instead of 101 for memory\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(model.fc.in_features, num_classes)\n",
    "        )\n",
    "        print(\"✅ Using ResNeXt50 (Memory Optimized)\")\n",
    "    elif model_name == 'densenet':\n",
    "        # Use smaller DenseNet\n",
    "        model = models.densenet121(pretrained=True)  # 121 instead of 161\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "        print(\"✅ Using DenseNet121 (Memory Optimized)\")\n",
    "    else:\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        print(\"✅ Using ResNeXt50 (Default)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Label smoothing loss (memory efficient)\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        confidence = 1. - self.smoothing\n",
    "        log_probs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        \n",
    "        if self.weight is not None:\n",
    "            loss = loss * self.weight[target]\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "# Memory-optimized class weights\n",
    "def get_class_weights(df, smoothing=0.1):\n",
    "    class_counts = df['label_idx'].value_counts().sort_index().values\n",
    "    total_samples = len(df)\n",
    "    smoothed_counts = class_counts + smoothing * total_samples / len(class_counts)\n",
    "    class_weights = total_samples / (len(smoothed_counts) * smoothed_counts)\n",
    "    class_weights = np.clip(class_weights, 0.7, 2.0)  # Less extreme weights\n",
    "    return torch.FloatTensor(class_weights)\n",
    "\n",
    "# Memory-efficient training function\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, (imgs, labels) in enumerate(pbar):\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "        \n",
    "        # Memory cleanup every 20 batches\n",
    "        if batch_idx % 20 == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}', \n",
    "                'Acc': f'{correct/total:.4f}',\n",
    "                'VRAM': f'{torch.cuda.memory_allocated()/1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'\n",
    "            })\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# Memory-efficient validation\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds_all, labels_all = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "            \n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Clean up\n",
    "            del imgs, labels, outputs\n",
    "    \n",
    "    f1 = f1_score(labels_all, preds_all, average=\"micro\")\n",
    "    return total_loss / total, correct / total, f1\n",
    "\n",
    "# Memory-efficient TTA validation (limited)\n",
    "def validate_with_tta(model, dataset, device, batch_size=8):  # Smaller batch for TTA\n",
    "    model.eval()\n",
    "    tta_transforms = get_tta_transforms(IMG_SIZE)\n",
    "    \n",
    "    all_preds = []\n",
    "    labels_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process in smaller chunks for memory\n",
    "        for start_idx in tqdm(range(0, len(dataset.df), batch_size), desc=\"TTA Validation\"):\n",
    "            end_idx = min(start_idx + batch_size, len(dataset.df))\n",
    "            batch_preds = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for idx in range(start_idx, end_idx):\n",
    "                row = dataset.df.iloc[idx]\n",
    "                img_path = os.path.join(dataset.img_dir, row['image_id'])\n",
    "                \n",
    "                try:\n",
    "                    with Image.open(img_path) as image:\n",
    "                        image = image.convert(\"RGB\")\n",
    "                        \n",
    "                        tta_preds = []\n",
    "                        for transform in tta_transforms:\n",
    "                            img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "                            output = model(img_tensor)\n",
    "                            pred = F.softmax(output, dim=1)\n",
    "                            tta_preds.append(pred.cpu().numpy())\n",
    "                            del img_tensor, output  # Immediate cleanup\n",
    "                        \n",
    "                        avg_pred = np.mean(tta_preds, axis=0)\n",
    "                        batch_preds.append(avg_pred)\n",
    "                        batch_labels.append(row['label_idx'])\n",
    "                except:\n",
    "                    # Fallback for failed images\n",
    "                    dummy_pred = np.ones((1, 20)) / 20\n",
    "                    batch_preds.append(dummy_pred)\n",
    "                    batch_labels.append(row['label_idx'])\n",
    "            \n",
    "            if batch_preds:\n",
    "                all_preds.extend(batch_preds)\n",
    "                labels_all.extend(batch_labels)\n",
    "            \n",
    "            # Memory cleanup after each batch\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    pred_classes = np.argmax(all_preds, axis=1)\n",
    "    \n",
    "    f1 = f1_score(labels_all, pred_classes, average=\"micro\")\n",
    "    accuracy = (pred_classes == np.array(labels_all)).mean()\n",
    "    \n",
    "    return f1, accuracy\n",
    "\n",
    "# Main training loop optimized for RTX 4060\n",
    "def train_models():\n",
    "    # Prioritize best performing models for limited compute\n",
    "    model_configs = [\n",
    "        {'name': 'efficientnet', 'lr': 3e-5, 'epochs': 16},\n",
    "        {'name': 'resnext101', 'lr': 2e-5, 'epochs': 14}, \n",
    "        {'name': 'densenet', 'lr': 4e-5, 'epochs': 15},\n",
    "    ]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for config in model_configs:\n",
    "        model_name = config['name']\n",
    "        lr = config['lr']\n",
    "        epochs = config['epochs']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Clear memory before each model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label_idx'])):\n",
    "            print(f\"\\n🔥 Fold {fold+1}/{N_SPLITS}\")\n",
    "            \n",
    "            train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "            val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}\")\n",
    "            \n",
    "            # Create datasets\n",
    "            train_dataset = MemoryEfficientDataset(\n",
    "                train_df, TRAIN_DIR, get_train_transforms(IMG_SIZE), is_training=True\n",
    "            )\n",
    "            val_dataset = MemoryEfficientDataset(\n",
    "                val_df, TRAIN_DIR, get_val_transforms(IMG_SIZE)\n",
    "            )\n",
    "            \n",
    "            # DataLoaders optimized for 4060\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                shuffle=True,\n",
    "                num_workers=2,  # Reduced for stability\n",
    "                pin_memory=True,\n",
    "                drop_last=True,\n",
    "                persistent_workers=True\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                shuffle=False,\n",
    "                num_workers=2,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True\n",
    "            )\n",
    "            \n",
    "            # Model setup\n",
    "            model = get_model(model_name, num_classes).to(DEVICE)\n",
    "            \n",
    "            # Memory-efficient mixed precision training\n",
    "            scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "            \n",
    "            # Optimizer\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(), \n",
    "                lr=lr, \n",
    "                weight_decay=1e-4 if model_name == 'efficientnet' else 5e-5,\n",
    "                eps=1e-8\n",
    "            )\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=epochs, eta_min=1e-7\n",
    "            )\n",
    "            \n",
    "            # Loss function\n",
    "            class_weights = get_class_weights(train_df).to(DEVICE)\n",
    "            criterion = LabelSmoothingCrossEntropy(smoothing=0.1, weight=class_weights)\n",
    "            \n",
    "            # Training loop\n",
    "            best_f1 = 0\n",
    "            patience = 5\n",
    "            patience_counter = 0\n",
    "            \n",
    "            print(\"Starting training...\")\n",
    "            for epoch in range(epochs):\n",
    "                print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "                \n",
    "                # Train\n",
    "                train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "                \n",
    "                # Validate\n",
    "                val_loss, val_acc, val_f1 = validate(model, val_loader, criterion, DEVICE)\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "                print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "                \n",
    "                # Save best model in organized folders\n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Use TTA for final validation on very good models\n",
    "                    if val_f1 > 0.88:  # Only for promising models to save time\n",
    "                        try:\n",
    "                            tta_f1, tta_acc = validate_with_tta(model, val_dataset, DEVICE, batch_size=4)\n",
    "                            print(f\"TTA - Acc: {tta_acc:.4f}, F1: {tta_f1:.4f}\")\n",
    "                            if tta_f1 > best_f1:\n",
    "                                best_f1 = tta_f1\n",
    "                        except Exception as e:\n",
    "                            print(f\"TTA failed: {e}, using regular validation\")\n",
    "                    \n",
    "                    # Save in organized folder structure\n",
    "                    model_folder = os.path.join(MODEL_SAVE_DIR, model_name)\n",
    "                    model_path = os.path.join(model_folder, f\"fold{fold}_best.pth\")\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'best_f1': best_f1,\n",
    "                        'epoch': epoch,\n",
    "                        'model_name': model_name,\n",
    "                        'fold': fold\n",
    "                    }, model_path)\n",
    "                    \n",
    "                    print(f\"✅ New best F1: {best_f1:.4f}\")\n",
    "                    print(f\"📁 Model saved: {model_path}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"⏰ Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "                \n",
    "                # Memory cleanup\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            fold_scores.append(best_f1)\n",
    "            print(f\"\\n📊 Fold {fold+1} completed - Best F1: {best_f1:.4f}\")\n",
    "            \n",
    "            # Clean up model for next fold\n",
    "            del model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        avg_score = np.mean(fold_scores)\n",
    "        std_score = np.std(fold_scores)\n",
    "        all_results[model_name] = {\n",
    "            'scores': fold_scores,\n",
    "            'mean': avg_score,\n",
    "            'std': std_score\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n🎯 {model_name} Results:\")\n",
    "        print(f\"Fold scores: {[f'{score:.4f}' for score in fold_scores]}\")\n",
    "        print(f\"Average F1: {avg_score:.4f} ± {std_score:.4f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Model loading utility\n",
    "def load_best_models():\n",
    "    \"\"\"Load the best models from each architecture\"\"\"\n",
    "    print(\"📁 Available models:\")\n",
    "    \n",
    "    for model_type in ['efficientnet', 'resnext101', 'densenet']:\n",
    "        model_folder = os.path.join(MODEL_SAVE_DIR, model_type)\n",
    "        if os.path.exists(model_folder):\n",
    "            model_files = [f for f in os.listdir(model_folder) if f.endswith('.pth')]\n",
    "            print(f\"\\n{model_type}:\")\n",
    "            for file in model_files:\n",
    "                full_path = os.path.join(model_folder, file)\n",
    "                try:\n",
    "                    checkpoint = torch.load(full_path, map_location='cpu')\n",
    "                    print(f\"  {file}: F1 = {checkpoint.get('best_f1', 'Unknown'):.4f}\")\n",
    "                except:\n",
    "                    print(f\"  {file}: Unable to load info\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting RTX 4060 optimized training...\")\n",
    "    print(f\"💾 Models will be saved in: {MODEL_SAVE_DIR}/\")\n",
    "    \n",
    "    results = train_models()\n",
    "    \n",
    "    print(\"\\n🎉 Training completed!\")\n",
    "    print(\"\\n📊 Final Results Summary:\")\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"{model_name}: {result['mean']:.4f} ± {result['std']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📁 All models saved in organized folders:\")\n",
    "    load_best_models()\n",
    "    \n",
    "    print(f\"\\n💡 Next steps:\")\n",
    "    print(\"1. Use the best models for ensemble predictions\")\n",
    "    print(\"2. Apply Test Time Augmentation on test set\")\n",
    "    print(\"3. Average predictions from multiple folds/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Configuration optimized for RTX 4060 (8GB VRAM)\n",
    "DATA_DIR = \"./\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train/train/\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test/test/\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "\n",
    "# Conservative settings to avoid hanging\n",
    "BATCH_SIZE = 12  # Reduced from 16\n",
    "EPOCHS = 15\n",
    "LR = 3e-5\n",
    "N_SPLITS = 5\n",
    "IMG_SIZE = 320\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "# Memory optimization\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Create model save directories\n",
    "MODEL_SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "for model_type in ['efficientnet', 'resnext', 'densenet']:\n",
    "    model_dir = os.path.join(MODEL_SAVE_DIR, model_type)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load and validate data\n",
    "print(\"Loading and validating data...\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['label_idx'] = le.fit_transform(df['TARGET'])\n",
    "df['image_id'] = df['ID']\n",
    "num_classes = df['label_idx'].nunique()\n",
    "\n",
    "# Validate that all image files exist\n",
    "print(\"Validating image files...\")\n",
    "valid_indices = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Checking files\"):\n",
    "    img_path = os.path.join(TRAIN_DIR, row['image_id'])\n",
    "    if os.path.exists(img_path):\n",
    "        try:\n",
    "            # Quick validation - try to open the image\n",
    "            with Image.open(img_path) as img:\n",
    "                img.verify()  # Check if image is corrupted\n",
    "            valid_indices.append(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping corrupted image: {img_path} - {e}\")\n",
    "    else:\n",
    "        print(f\"Missing file: {img_path}\")\n",
    "\n",
    "# Keep only valid images\n",
    "df = df.iloc[valid_indices].reset_index(drop=True)\n",
    "print(f\"Valid images: {len(df)} / {len(valid_indices)}\")\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Class distribution:\")\n",
    "print(df['TARGET'].value_counts().head())\n",
    "\n",
    "# Robust dataset with better error handling\n",
    "class RobustDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, is_training=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # Cache image paths for faster access\n",
    "        self.image_paths = [os.path.join(img_dir, row['image_id']) for _, row in df.iterrows()]\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.df)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                row = self.df.iloc[idx]\n",
    "                img_path = self.image_paths[idx]\n",
    "                \n",
    "                # Load image with timeout protection\n",
    "                with Image.open(img_path) as image:\n",
    "                    image = image.convert(\"RGB\")\n",
    "                    \n",
    "                    # Apply transform immediately while image is loaded\n",
    "                    if self.transform:\n",
    "                        image = self.transform(image)\n",
    "                    else:\n",
    "                        # Fallback transform\n",
    "                        image = transforms.ToTensor()(image)\n",
    "                \n",
    "                label = row['label_idx']\n",
    "                return image, label\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {idx} (attempt {attempt+1}): {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    # Return a black image as fallback\n",
    "                    print(f\"Using fallback for image {idx}\")\n",
    "                    fallback_img = torch.zeros((3, IMG_SIZE, IMG_SIZE))\n",
    "                    return fallback_img, row['label_idx']\n",
    "                time.sleep(0.1)  # Brief pause before retry\n",
    "\n",
    "# Simplified transforms to reduce processing time\n",
    "def get_train_transforms(img_size=320):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(img_size=320):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "# Simplified model loading\n",
    "def get_model(model_name='efficientnet', num_classes=20):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    if model_name == 'efficientnet':\n",
    "        model = models.efficientnet_b1(pretrained=True)  # B1 instead of B2 for more stability\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        print(\"✅ Using EfficientNet-B1\")\n",
    "    elif model_name == 'resnext':\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        print(\"✅ Using ResNeXt50\")\n",
    "    elif model_name == 'densenet':\n",
    "        model = models.densenet121(pretrained=True)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "        print(\"✅ Using DenseNet121\")\n",
    "    else:\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        print(\"✅ Using ResNet50 (fallback)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Simplified loss function\n",
    "def get_class_weights(df):\n",
    "    class_counts = df['label_idx'].value_counts().sort_index().values\n",
    "    total_samples = len(df)\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    class_weights = np.clip(class_weights, 0.5, 2.0)\n",
    "    return torch.FloatTensor(class_weights)\n",
    "\n",
    "# Robust training function\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"Starting epoch with {len(loader)} batches...\")\n",
    "    \n",
    "    # Force CUDA initialization\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", total=len(loader))\n",
    "    \n",
    "    batch_count = 0\n",
    "    for batch_idx, (imgs, labels) in enumerate(pbar):\n",
    "        try:\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Move to device\n",
    "            imgs = imgs.to(device, non_blocking=False)  # Disable non_blocking for stability\n",
    "            labels = labels.to(device, non_blocking=False)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "            \n",
    "            # Update progress\n",
    "            if batch_idx % 5 == 0:\n",
    "                current_acc = correct / total if total > 0 else 0\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}', \n",
    "                    'Acc': f'{current_acc:.3f}',\n",
    "                    'Batch': f'{batch_idx+1}/{len(loader)}'\n",
    "                })\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_loss / total if total > 0 else 0, correct / total if total > 0 else 0\n",
    "\n",
    "# Simplified validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds_all, labels_all = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "        for imgs, labels in pbar:\n",
    "            try:\n",
    "                imgs = imgs.to(device, non_blocking=False)\n",
    "                labels = labels.to(device, non_blocking=False)\n",
    "                \n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item() * imgs.size(0)\n",
    "                preds = outputs.argmax(1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += imgs.size(0)\n",
    "                \n",
    "                preds_all.extend(preds.cpu().numpy())\n",
    "                labels_all.extend(labels.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in validation batch: {e}\")\n",
    "                continue\n",
    "        pbar.close()\n",
    "    \n",
    "    f1 = f1_score(labels_all, preds_all, average=\"micro\") if len(preds_all) > 0 else 0\n",
    "    return total_loss / total if total > 0 else 0, correct / total if total > 0 else 0, f1\n",
    "\n",
    "# Main training function with better error handling\n",
    "def train_models():\n",
    "    model_configs = [\n",
    "        {'name': 'efficientnet', 'lr': 3e-5, 'epochs': 15},\n",
    "        {'name': 'resnext', 'lr': 2e-5, 'epochs': 12}, \n",
    "        {'name': 'densenet', 'lr': 4e-5, 'epochs': 14},\n",
    "    ]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for config in model_configs:\n",
    "        model_name = config['name']\n",
    "        lr = config['lr']\n",
    "        epochs = config['epochs']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label_idx'])):\n",
    "            print(f\"\\n🔥 Fold {fold+1}/{N_SPLITS}\")\n",
    "            \n",
    "            train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "            val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}\")\n",
    "            \n",
    "            # Create datasets\n",
    "            train_dataset = RobustDataset(\n",
    "                train_df, TRAIN_DIR, get_train_transforms(IMG_SIZE), is_training=True\n",
    "            )\n",
    "            val_dataset = RobustDataset(\n",
    "                val_df, TRAIN_DIR, get_val_transforms(IMG_SIZE)\n",
    "            )\n",
    "            \n",
    "            # Create robust data loaders\n",
    "            print(\"Creating data loaders...\")\n",
    "            try:\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=True,\n",
    "                    num_workers=0,  # Disable multiprocessing\n",
    "                    pin_memory=False,  # Disable memory pinning\n",
    "                    drop_last=True,\n",
    "                    timeout=0,  # No timeout\n",
    "                    persistent_workers=False\n",
    "                )\n",
    "                val_loader = DataLoader(\n",
    "                    val_dataset, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=False,\n",
    "                    num_workers=0,\n",
    "                    pin_memory=False,\n",
    "                    timeout=0,\n",
    "                    persistent_workers=False\n",
    "                )\n",
    "                \n",
    "                print(f\"✅ DataLoaders created - Train: {len(train_loader)} batches\")\n",
    "                \n",
    "                # Test first batch\n",
    "                print(\"Testing first batch...\")\n",
    "                test_batch = next(iter(train_loader))\n",
    "                print(f\"✅ First batch loaded: {test_batch[0].shape}\")\n",
    "                del test_batch\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ DataLoader creation failed: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Model setup\n",
    "            print(f\"Setting up {model_name} model...\")\n",
    "            try:\n",
    "                model = get_model(model_name, num_classes).to(DEVICE)\n",
    "                \n",
    "                # Test model\n",
    "                dummy_input = torch.randn(2, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    dummy_output = model(dummy_input)\n",
    "                print(f\"✅ Model test passed: {dummy_output.shape}\")\n",
    "                del dummy_input, dummy_output\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Model setup failed: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Optimizer and scheduler\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7)\n",
    "            \n",
    "            # Loss function\n",
    "            class_weights = get_class_weights(train_df).to(DEVICE)\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "            \n",
    "            # Training loop\n",
    "            best_f1 = 0\n",
    "            patience = 4\n",
    "            patience_counter = 0\n",
    "            \n",
    "            print(\"Starting training loop...\")\n",
    "            for epoch in range(epochs):\n",
    "                print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "                \n",
    "                try:\n",
    "                    # Train\n",
    "                    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "                    \n",
    "                    # Validate\n",
    "                    val_loss, val_acc, val_f1 = validate(model, val_loader, criterion, DEVICE)\n",
    "                    \n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "                    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "                    \n",
    "                    # Save best model\n",
    "                    if val_f1 > best_f1:\n",
    "                        best_f1 = val_f1\n",
    "                        patience_counter = 0\n",
    "                        \n",
    "                        model_folder = os.path.join(MODEL_SAVE_DIR, model_name)\n",
    "                        model_path = os.path.join(model_folder, f\"fold{fold}_best.pth\")\n",
    "                        torch.save({\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'best_f1': best_f1,\n",
    "                            'epoch': epoch,\n",
    "                            'model_name': model_name,\n",
    "                            'fold': fold\n",
    "                        }, model_path)\n",
    "                        \n",
    "                        print(f\"✅ New best F1: {best_f1:.4f} - Model saved!\")\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            print(f\"⏰ Early stopping at epoch {epoch+1}\")\n",
    "                            break\n",
    "                    \n",
    "                    # Memory cleanup\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error in epoch {epoch+1}: {e}\")\n",
    "                    break\n",
    "            \n",
    "            fold_scores.append(best_f1)\n",
    "            print(f\"\\n📊 Fold {fold+1} completed - Best F1: {best_f1:.4f}\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del model, optimizer, scheduler, criterion\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        if fold_scores:\n",
    "            avg_score = np.mean(fold_scores)\n",
    "            std_score = np.std(fold_scores)\n",
    "            all_results[model_name] = {\n",
    "                'scores': fold_scores,\n",
    "                'mean': avg_score,\n",
    "                'std': std_score\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n🎯 {model_name} Results:\")\n",
    "            print(f\"Fold scores: {[f'{score:.4f}' for score in fold_scores]}\")\n",
    "            print(f\"Average F1: {avg_score:.4f} ± {std_score:.4f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting robust training for RTX 4060...\")\n",
    "    print(f\"💾 Models will be saved in: {MODEL_SAVE_DIR}/\")\n",
    "    \n",
    "    try:\n",
    "        results = train_models()\n",
    "        \n",
    "        print(\"\\n🎉 Training completed!\")\n",
    "        print(\"\\n📊 Final Results Summary:\")\n",
    "        for model_name, result in results.items():\n",
    "            print(f\"{model_name}: {result['mean']:.4f} ± {result['std']:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n📁 Check {MODEL_SAVE_DIR}/ for saved models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb5713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
